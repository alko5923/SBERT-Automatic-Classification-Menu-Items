{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exjobb-preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook pre-processes the data that will be used for the thesis, and computes sentence embeddings for different models, which it then saves to csv files. \n",
        "\n",
        "It uses the data that was provided by Caspeco AB. First we calculate and save embeddings for completely unprocessed data ('original-1.csv'), and then we calculate and save embeddings for data which has been pre-processed ('processed-1.csv')."
      ],
      "metadata": {
        "id": "uXhemHy_DkBn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-PW98tmBfbA"
      },
      "outputs": [],
      "source": [
        "pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pre-processing (by hand) involved removing numerical quantities from certain items' names, e.g., 'Andiamo White 18cl', 'Falcon Export 50 cl', 'Buffalo Wings 5 pcs', etc. Then the file (see code below) was read into a pandas dataframe and duplicate entries were removed. Lastly, all item names were lowercased (since one of the models we will be using is cased, meaning it differentiates between, for example, \"English\" and \"english\")."
      ],
      "metadata": {
        "id": "YW9BORhrBuXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Change this line if you want to display more than 10 rows when printing\n",
        "pd.set_option('display.max_rows', 10)\n",
        "\n",
        "df_original = pd.read_csv('/content/original-1.csv')\n",
        "df_processed = pd.read_csv('/content/processed-1.csv')\n",
        "#Drop the category column\n",
        "df_original = df_original.drop(columns=['ArticleGroupName'])\n",
        "df_processed = df_processed.drop(columns=['ArticleGroupName'])\n",
        "\n",
        "\n",
        "# Inspect the data\n",
        "print(\"Df_orig value counts = \\n\", df_original.value_counts(), \"\\n\")\n",
        "print(\"Df_proc value counts = \\n\", df_processed.value_counts(), \"\\n\")\n",
        "\n",
        "\n",
        "# Remove duplicate rows based on all columns\n",
        "df_original = df_original.drop_duplicates(ignore_index=True)\n",
        "df_processed = df_processed.drop_duplicates(ignore_index=True)\n",
        "\n",
        "\n",
        "# Inspect the data again\n",
        "print(\"Df_original with duplicates removed = \\n\", df_original.value_counts(), \"\\n\")\n",
        "print(\"Df_processed with duplicates removed = \\n\", df_processed.value_counts(), \"\\n\")\n",
        "\n",
        "# Lowercase the items in the processed df\n",
        "df_proc_copy = df_processed.copy()\n",
        "lowercased = df_proc_copy['ArticleName'].str.strip().str.lower()\n",
        "df_processed['ArticleName'] = lowercased\n",
        "print(\"Processed df = \\n\", df_processed)\n",
        "print(\"Original df = \\n\", df_original)\n"
      ],
      "metadata": {
        "id": "tWEOmijcB-fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us calculate the embeddings for the items. We will be using 3 different, pre-trained models, all of them multilingual since our data includes English, Swedish and Spanish item names."
      ],
      "metadata": {
        "id": "6HMvmxpvGstJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\"\"\"\n",
        "Create embeddings for the items.\n",
        "Models: \n",
        "'distiluse-base-multilingual-cased-v2',  \n",
        "'paraphrase-multilingual-MiniLM-L12-v2' and\n",
        "'paraphrase-multilingual-mpnet-base-v2'.\n",
        "\"\"\"\n",
        "# Define the models\n",
        "model_distiluse = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
        "model_para_mini = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "model_para_base = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
        "\n",
        "# Read items from the file into a list\n",
        "items_original = df_original['ArticleName'].tolist()\n",
        "items_processed = df_processed['ArticleName'].tolist()\n",
        "\n",
        "# Calculate embeddings\n",
        "orig_embeddings_distiluse = model_distiluse.encode(items_original)\n",
        "orig_embeddings_para_mini = model_para_mini.encode(items_original)\n",
        "orig_embeddings_para_base = model_para_base.encode(items_original)\n",
        "\n",
        "proc_embeddings_distiluse = model_distiluse.encode(items_processed)\n",
        "proc_embeddings_para_mini = model_para_mini.encode(items_processed)\n",
        "proc_embeddings_para_base = model_para_base.encode(items_processed)\n",
        "\n"
      ],
      "metadata": {
        "id": "axK8emytGBXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now save the embeddings to csv files so we don't have to calculate them again in the future. "
      ],
      "metadata": {
        "id": "WKpSr9U5ITYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\"\"\"\n",
        "Save the embeddings of the 'distiluse-base-multilingual-cased-v2' model, both with original \n",
        "and pre-processed data.\n",
        "\"\"\"\n",
        "\n",
        "# Create names of features \n",
        "features_names = []\n",
        "for i in range(len(orig_embeddings_distiluse[0])):\n",
        "  name = 'f' + str(i+1)\n",
        "  features_names.append(name)\n",
        "\n",
        "# Stringify the vector elements, so you can concatenate them later with item names\n",
        "orig_emb_distiluse_str = []\n",
        "for emb in orig_embeddings_distiluse:\n",
        "  emb_str = [str(x) for x in emb]\n",
        "  orig_emb_distiluse_str.append(emb_str)\n",
        "\n",
        "# Stringify the vector elements, so you can concatenate them later with item names\n",
        "proc_emb_distiluse_str = []\n",
        "for emb in proc_embeddings_distiluse:\n",
        "  emb_str = [str(x) for x in emb]\n",
        "  proc_emb_distiluse_str.append(emb_str)\n",
        "\n",
        "\n",
        "# Write to csv files\n",
        "with open('orig_1_emb_distiluse.csv', 'w', newline='') as csvfile:\n",
        "    spamwriter = csv.writer(csvfile)\n",
        "    spamwriter.writerow(['item_orig'] + features_names)\n",
        "    for i in range(len(items_original)):\n",
        "      spamwriter.writerow([items_original[i]] + orig_emb_distiluse_str[i])\n",
        "\n",
        "with open('proc_1_emb_distiluse.csv', 'w', newline='') as csvfile:\n",
        "    spamwriter = csv.writer(csvfile)\n",
        "    spamwriter.writerow(['item_proc'] + features_names)\n",
        "    for i in range(len(items_processed)):\n",
        "      spamwriter.writerow([items_processed[i]] + proc_emb_distiluse_str[i])\n"
      ],
      "metadata": {
        "id": "jLe5J_RyIYDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\"\"\"\n",
        "Save the embeddings of the 'paraphrase-multilingual-MiniLM-L12-v2' model, \n",
        "both with original and pre-processed data.\n",
        "\"\"\"\n",
        "\n",
        "# Create names of features \n",
        "features_names = []\n",
        "for i in range(len(orig_embeddings_para_mini[0])):\n",
        "  name = 'f' + str(i+1)\n",
        "  features_names.append(name)\n",
        "\n",
        "# Stringify the vector elements, so you can concatenate them later with item names\n",
        "orig_emb_para_mini_str = []\n",
        "for emb in orig_embeddings_para_mini:\n",
        "  emb_str = [str(x) for x in emb]\n",
        "  orig_emb_para_mini_str.append(emb_str)\n",
        "\n",
        "# Stringify the vector elements, so you can concatenate them later with item names\n",
        "proc_emb_para_mini_str = []\n",
        "for emb in proc_embeddings_para_mini:\n",
        "  emb_str = [str(x) for x in emb]\n",
        "  proc_emb_para_mini_str.append(emb_str)\n",
        "\n",
        "\n",
        "# Write to csv files\n",
        "with open('orig_1_emb_para_mini.csv', 'w', newline='') as csvfile:\n",
        "    spamwriter = csv.writer(csvfile)\n",
        "    spamwriter.writerow(['item_orig'] + features_names)\n",
        "    for i in range(len(items_original)):\n",
        "      spamwriter.writerow([items_original[i]] + orig_emb_para_mini_str[i])\n",
        "\n",
        "with open('proc_1_emb_para_mini.csv', 'w', newline='') as csvfile:\n",
        "    spamwriter = csv.writer(csvfile)\n",
        "    spamwriter.writerow(['item_proc'] + features_names)\n",
        "    for i in range(len(items_processed)):\n",
        "      spamwriter.writerow([items_processed[i]] + proc_emb_para_mini_str[i])\n"
      ],
      "metadata": {
        "id": "6UbEqLUiK6i2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\"\"\"\n",
        "Save the embeddings of the 'paraphrase-multilingual-mpnet-base-v2' model, \n",
        "both with original and pre-processed data.\n",
        "\"\"\"\n",
        "\n",
        "# Create names of features \n",
        "features_names = []\n",
        "for i in range(len(orig_embeddings_para_base[0])):\n",
        "  name = 'f' + str(i+1)\n",
        "  features_names.append(name)\n",
        "\n",
        "# Stringify the vector elements, so you can concatenate them later with item names\n",
        "orig_emb_para_base_str = []\n",
        "for emb in orig_embeddings_para_base:\n",
        "  emb_str = [str(x) for x in emb]\n",
        "  orig_emb_para_base_str.append(emb_str)\n",
        "\n",
        "# Stringify the vector elements, so you can concatenate them later with item names\n",
        "proc_emb_para_base_str = []\n",
        "for emb in proc_embeddings_para_base:\n",
        "  emb_str = [str(x) for x in emb]\n",
        "  proc_emb_para_base_str.append(emb_str)\n",
        "\n",
        "\n",
        "# Write to csv files\n",
        "with open('orig_1_emb_para_base.csv', 'w', newline='') as csvfile:\n",
        "    spamwriter = csv.writer(csvfile)\n",
        "    spamwriter.writerow(['item_orig'] + features_names)\n",
        "    for i in range(len(items_original)):\n",
        "      spamwriter.writerow([items_original[i]] + orig_emb_para_base_str[i])\n",
        "\n",
        "with open('proc_1_emb_para_base.csv', 'w', newline='') as csvfile:\n",
        "    spamwriter = csv.writer(csvfile)\n",
        "    spamwriter.writerow(['item_proc'] + features_names)\n",
        "    for i in range(len(items_processed)):\n",
        "      spamwriter.writerow([items_processed[i]] + proc_emb_para_base_str[i])\n"
      ],
      "metadata": {
        "id": "Xp2kWY_eQmjg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}