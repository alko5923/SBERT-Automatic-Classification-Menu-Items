{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2356ab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: This framework generates embeddings for each input sentence\n",
      "Embedding: [-1.76214471e-01  1.20601423e-01 -2.93623894e-01 -2.29858086e-01\n",
      " -8.22924674e-02  2.37709492e-01  3.39985222e-01 -7.80964196e-01\n",
      "  1.18127503e-01  1.63373962e-01 -1.37715206e-01  2.40282550e-01\n",
      "  4.25125659e-01  1.72418088e-01  1.05279632e-01  5.18164039e-01\n",
      "  6.22218512e-02  3.99285495e-01 -1.81652248e-01 -5.85578799e-01\n",
      "  4.49720435e-02 -1.72750235e-01 -2.68443555e-01 -1.47386104e-01\n",
      " -1.89218029e-01  1.92150757e-01 -3.83842319e-01 -3.96007061e-01\n",
      "  4.30648863e-01 -3.15319747e-01  3.65949780e-01  6.05159029e-02\n",
      "  3.57325673e-01  1.59736469e-01 -3.00983876e-01  2.63250172e-01\n",
      " -3.94311070e-01  1.84855551e-01 -3.99549127e-01 -2.67889470e-01\n",
      " -5.45117080e-01 -3.13406326e-02 -4.30644304e-01  1.33278280e-01\n",
      " -1.74793869e-01 -4.35465455e-01 -4.77379113e-01  7.12556392e-02\n",
      " -7.37002343e-02  5.69136918e-01 -2.82579750e-01  5.24974838e-02\n",
      " -8.20007920e-01  1.98296934e-01  1.69511974e-01  2.71780193e-01\n",
      "  2.64611065e-01 -2.55738590e-02 -1.74096555e-01  1.63314283e-01\n",
      " -3.95260751e-01 -3.17559019e-02 -2.62556046e-01  3.52754563e-01\n",
      "  3.01434755e-01 -1.47197217e-01  2.10075840e-01 -1.84010714e-01\n",
      " -4.12895888e-01  4.14775819e-01 -1.89769372e-01 -1.35482222e-01\n",
      " -3.79272312e-01 -4.68023084e-02 -3.33603546e-02  9.00395215e-02\n",
      " -3.30132842e-01 -3.87319550e-02  3.75082314e-01 -1.46996409e-01\n",
      "  4.34959620e-01  5.38325906e-01 -2.65445322e-01  1.64445728e-01\n",
      "  4.17078614e-01 -4.72509116e-02 -7.48732537e-02 -4.26260799e-01\n",
      " -1.96994409e-01  6.10317513e-02 -4.74262744e-01 -6.48334622e-01\n",
      "  3.71462524e-01  2.50956953e-01  1.22529618e-01  8.88767168e-02\n",
      " -1.06724575e-01  5.33983186e-02  9.74506214e-02 -3.46660912e-02\n",
      " -1.02882914e-01  2.32288852e-01 -2.53739655e-01 -5.13112426e-01\n",
      "  1.85216203e-01 -3.04357648e-01 -3.55212055e-02 -1.26975283e-01\n",
      " -7.71633759e-02 -5.15330076e-01 -2.28072017e-01  2.03346685e-02\n",
      "  7.38177001e-02 -1.52558640e-01 -4.00837898e-01 -2.47749224e-01\n",
      "  3.97470146e-01 -2.60260552e-01  2.50906199e-01  1.68228999e-01\n",
      "  1.33900478e-01 -2.10833140e-02 -4.70035344e-01  4.78850305e-01\n",
      "  2.80345589e-01 -4.64546800e-01  3.21746945e-01  2.34207153e-01\n",
      "  2.45772496e-01 -4.71482247e-01  5.00401378e-01  4.10190105e-01\n",
      "  5.15216887e-01  2.62549609e-01  2.11592447e-02 -3.89687330e-01\n",
      " -2.41742671e-01 -2.14834630e-01 -8.62650052e-02 -1.65323257e-01\n",
      " -5.21891750e-02  3.41874957e-01  4.50314313e-01 -3.06973606e-01\n",
      " -2.02294260e-01  6.85521781e-01 -5.33892632e-01  3.58471334e-01\n",
      "  1.45286441e-01 -7.07055703e-02 -1.50529236e-01 -8.56282413e-02\n",
      " -7.67848566e-02  1.89544752e-01 -1.04067720e-01  5.33544004e-01\n",
      " -5.27886987e-01  2.42332388e-02 -2.64347821e-01 -2.23186836e-01\n",
      " -3.81208837e-01  7.59916231e-02 -4.64484990e-01 -3.36549014e-01\n",
      "  4.21229720e-01  1.07479200e-01  1.90457836e-01  2.89471308e-03\n",
      " -1.08513720e-01  1.53545409e-01  3.16023290e-01 -2.70837825e-02\n",
      " -5.40594697e-01  8.97288024e-02 -1.15549870e-01  3.97803783e-01\n",
      " -4.97683197e-01 -2.84893453e-01  4.99863699e-02  3.61279517e-01\n",
      "  6.90535545e-01  1.46821603e-01  1.73396587e-01 -1.74582317e-01\n",
      " -3.15702319e-01  6.73001930e-02  2.17250124e-01  9.78533775e-02\n",
      " -1.29472390e-01 -1.86929613e-01  1.34878054e-01 -1.53885201e-01\n",
      "  7.44716004e-02 -1.85536176e-01 -2.80628324e-01 -1.14144072e-01\n",
      "  4.12249774e-01  6.39494509e-02 -1.45715177e-01 -9.82059911e-02\n",
      " -1.33081809e-01 -1.88410759e-01 -2.84842066e-02 -3.49510610e-02\n",
      "  3.34258117e-02  6.98897913e-02  1.90354332e-01 -2.96724081e-01\n",
      "  2.64698383e-03  1.09140873e-01  1.70893986e-02  2.60589540e-01\n",
      "  3.29038650e-01 -6.61559477e-02  2.39665434e-01 -2.26194724e-01\n",
      " -3.36868241e-02  1.49400219e-01 -3.21265697e-01 -2.68577993e-01\n",
      "  5.72631836e-01 -4.92308557e-01  2.00666681e-01 -3.49261612e-01\n",
      " -2.89888345e-02  6.09010398e-01 -5.72332978e-01  2.35000387e-01\n",
      "  6.47182995e-03 -3.14954072e-02  2.78106891e-02 -3.90340596e-01\n",
      " -2.08950073e-01 -3.04452717e-01 -7.20200837e-02 -8.29840004e-02\n",
      "  3.73792946e-01  7.38936514e-02 -2.21074633e-02  9.88140404e-02\n",
      " -1.51426926e-01 -1.40431106e-01  2.26018116e-01  2.76090324e-01\n",
      " -8.87751579e-02 -1.12815931e-01 -2.66286165e-01  2.77834445e-01\n",
      " -4.75611538e-02  6.71004727e-02 -2.78583877e-02 -2.39993464e-02\n",
      "  2.51708627e-01  4.68793839e-01 -5.39325595e-01  1.10598370e-01\n",
      " -3.44947517e-01  4.15989876e-01  7.28482008e-02 -3.19647610e-01\n",
      "  4.90374357e-01 -7.30358856e-03 -2.64246133e-03  9.63711023e-01\n",
      "  3.23884964e-01 -7.79617876e-02 -2.37589195e-01  2.34038651e-01\n",
      " -3.16054195e-01 -1.65654148e-03 -1.09070659e+00  3.38409215e-01\n",
      "  4.70608063e-02  1.07435390e-01 -2.06672445e-01  4.26432723e-03\n",
      " -1.38485373e-03 -5.31455755e-01 -2.75648534e-01 -1.64648712e-01\n",
      " -3.42916429e-01 -4.26119059e-01  6.01811886e-01  4.55971897e-01\n",
      " -2.72701740e-01 -3.45805101e-02  2.62752444e-01 -6.34190766e-03\n",
      "  2.79631108e-01 -2.53559083e-01 -1.68626398e-01  3.82934734e-02\n",
      "  2.07763106e-01 -4.31526005e-01 -7.24000856e-02 -1.26854196e-01\n",
      "  2.07028501e-02  5.74441433e-01  3.54672372e-01  9.28303078e-02\n",
      "  6.70507103e-02  1.11520685e-01 -1.86511353e-02  4.62352157e-01\n",
      "  2.72504896e-01 -3.60473931e-01  5.29415548e-01 -1.00291625e-03\n",
      " -8.81361440e-02  1.49975687e-01  5.25864027e-02  4.63517547e-01\n",
      " -3.96831542e-01  2.42640793e-01 -2.08912373e-01  3.65672231e-01\n",
      " -4.73651162e-04  5.33963382e-01 -1.97879404e-01  3.11582983e-01\n",
      " -6.96715057e-01 -4.29500401e-01 -4.49359536e-01 -2.71368995e-02\n",
      " -6.98710233e-02  2.06174389e-01 -1.57107830e-01  4.43521321e-01\n",
      " -6.74268529e-02 -3.00923914e-01  5.14859259e-01  3.36029708e-01\n",
      "  6.63378313e-02 -1.15235202e-01 -2.95981094e-02  2.79471666e-01\n",
      " -3.48202512e-02 -7.29323551e-02 -4.58472930e-02  1.54262811e-01\n",
      "  8.09356153e-01  5.20327985e-01 -4.02114511e-01 -3.23154069e-02\n",
      " -1.10364228e-01  7.50503018e-02 -1.51098505e-01  8.45740020e-01\n",
      " -1.80844054e-01  3.22573513e-01  1.04708023e-01  3.19663852e-01\n",
      " -1.55085340e-01  1.69236645e-01 -2.56996483e-01  2.01208830e-01\n",
      "  1.77393287e-01 -2.74333179e-01 -3.36944282e-01  5.02356768e-01\n",
      " -1.18357293e-01 -2.01167032e-01 -5.36485553e-01 -7.69809484e-02\n",
      "  1.15381535e-02 -2.36464605e-01 -2.98771001e-02  1.31366760e-01\n",
      "  2.94184297e-01  9.90919918e-02 -5.43897867e-01  1.40812770e-01\n",
      "  3.66998821e-01  5.04862480e-02  1.99122608e-01 -2.80674458e-01\n",
      "  4.34192061e-01 -1.40275151e-01  5.78048646e-01  1.77715719e-01\n",
      "  8.98365080e-02  3.29651624e-01  6.13009147e-02 -3.24933499e-01]\n",
      "\n",
      "Sentence: Sentences are passed as a list of string.\n",
      "Embedding: [ 0.3220874  -0.00123929  0.17937385 -0.36919153 -0.06460265  0.09153703\n",
      "  0.24119078 -0.294942    0.07728961  0.11577012 -0.04479983  0.17928249\n",
      "  0.14753616  0.2151164   0.3681078   0.20910908  0.2719424   0.34880075\n",
      " -0.57251924 -0.18253222  0.44489563  0.27452967  0.0426628  -0.07683557\n",
      "  0.18689126  0.4496505  -0.16932608 -0.24896345 -0.2047926   0.4028505\n",
      " -0.21019253  0.03775699  0.0784852   0.12848464  0.02593089  0.4715597\n",
      "  0.1785379  -0.07379749  0.08130738 -0.23328756 -0.49801263 -0.041357\n",
      " -0.12094576  0.17028992 -0.19154078 -0.38459834 -0.7747915  -0.10622726\n",
      " -0.23044871  0.4024145  -0.8745089   0.23853722 -0.47129887  0.21262197\n",
      "  0.3340933  -0.24154021 -0.14835101 -0.14513569 -0.34830925 -0.08349215\n",
      " -0.6909727  -0.29845265 -0.1223048   0.07482656 -0.18775629 -0.37546492\n",
      "  0.2136951  -0.10096409 -0.12234445  0.31431505 -0.23989934  0.22460757\n",
      "  0.03995967  0.36034855 -0.5663802   0.21883516  0.11020296 -0.10870826\n",
      "  0.07084081 -0.02608196  0.18370323  0.08465951 -0.2047823  -0.24435592\n",
      " -0.08180564 -0.01903092 -0.03591377  0.02398453 -0.28558576  0.07374787\n",
      " -0.2974422  -0.87717843  0.47101927 -0.04940467  0.36394474  0.482644\n",
      "  0.01564621  0.03558899 -0.26202983 -0.11218477  0.02411024  0.3747777\n",
      " -0.09897298 -0.0985186   0.15000835  0.00689531 -0.12652445 -0.31598923\n",
      "  0.31449535 -0.2942561  -0.26941028  0.20221168  0.14329886 -0.19584633\n",
      " -0.34104446 -0.03172754  0.7365027   0.31923485  0.24381317  0.30732605\n",
      "  0.09933242  0.19010936 -0.10694522  0.05178672  0.03233435 -0.10314655\n",
      "  0.2649923   0.3120647   0.43152606 -0.64261216  0.08409579 -0.04327349\n",
      " -0.04991212 -0.1271856   0.13789195  0.01306237  0.34383243  0.09234285\n",
      " -0.09922747 -0.52159923  0.25842246 -0.01057147 -0.00478159  0.03938847\n",
      "  0.19086093  0.32933876 -0.24345168 -0.07328343 -0.39280042  0.14541794\n",
      "  0.32839534 -0.04184618  0.07407132 -0.7386053  -0.09076001  0.1580231\n",
      " -0.09780025 -0.21605973 -0.30027488  0.2323657   0.0107244   0.49570456\n",
      "  0.04974838  0.29931405 -0.05382255  0.353281    0.3419176   0.49667245\n",
      " -0.4860526  -0.1909884   0.8154573   0.22962622 -0.320778   -0.3272671\n",
      " -0.36771697  0.34521177 -0.02620158 -0.14315067  0.1064842  -0.24638022\n",
      " -0.0936662   0.17198658 -0.08508798  0.20120281 -0.05879239 -0.34020993\n",
      " -0.19565333  0.28280887  0.20124318 -0.08207273  0.09779117 -0.26375002\n",
      "  0.12176557 -0.01041467 -0.43859807  0.11058246  0.4801038  -0.10981979\n",
      " -0.63754576  0.29336774 -0.1920763   0.46536997  0.27042004  0.19388486\n",
      "  0.17379008 -0.30076975 -0.02751208 -0.02291268  0.3678465   0.02492185\n",
      "  0.5370549   0.18851241 -0.13344426  0.08917329  0.05542953 -0.24818327\n",
      " -0.04199767  0.05767383 -0.18278822 -0.41686466  0.1607059  -0.46362513\n",
      "  0.11769219 -0.37706912  0.02960369  0.69256085 -0.48308936  0.21128386\n",
      "  0.18214537 -0.18429609  0.06817651 -0.02460896 -0.19073606 -0.06736993\n",
      " -0.5670072  -0.23929328 -0.08497205  0.03093945  0.3107991   0.1291628\n",
      "  0.05248232 -0.3344982   0.1881013   0.23547158 -0.00183493  0.45361605\n",
      "  0.24885081 -0.05641085 -0.2977459  -0.43511724 -0.07969428 -0.17670156\n",
      " -0.13347112  0.19382735  0.22002597 -0.11057539  0.26473734 -0.27179068\n",
      "  0.03410852 -0.47714424  0.44719046 -0.05570405  0.39643756  0.27483252\n",
      "  0.33305633 -0.10890245  0.27888167  0.21596943 -0.05252254 -0.35867563\n",
      " -0.69062936  0.03960198  0.00652785 -0.01095324 -0.10027688  0.04770019\n",
      " -0.34146944 -0.16714178  0.07136453 -0.18078464 -0.3024849  -0.6842873\n",
      " -0.09592854 -0.21411094 -0.65524393  0.5675645   0.26946735 -0.00190056\n",
      "  0.86180645  0.16771549  0.03102763 -0.2677304  -0.07830298 -0.48510897\n",
      " -0.26737216 -0.3335424  -0.5738251   0.35678247  0.08993593 -0.13057193\n",
      " -0.1513649  -0.06124127 -0.13037087  0.55856085  0.61417484 -0.04804039\n",
      " -0.06388561  0.08390608 -0.25143683 -0.04359853 -0.18525814  0.04693384\n",
      " -0.34380847 -0.09738454  0.16833638  0.07526854  0.17694505  0.17727174\n",
      " -0.03423443  0.14993578 -0.1377316  -0.2094968  -0.61272836  0.3781395\n",
      "  0.39018285 -0.08359315  0.03152138  0.1312239   0.38826057  0.21844243\n",
      "  0.09724302  0.42089346 -0.32641265 -0.26933435 -0.39095125 -0.22648674\n",
      " -0.32020724 -0.16287431 -0.03581639  0.36373883  0.18583323 -0.02914011\n",
      " -0.4657795   0.29168904  0.372513   -0.23726608  0.00338609  0.4154096\n",
      "  0.03300427  0.4500396  -0.08159241  0.3399035   0.24497886  0.023524\n",
      " -0.14643048 -0.12644547  0.31128657 -0.1518262   0.01009408  0.49108523\n",
      "  0.14362417  0.11589034 -0.23236981  0.24751748  0.18364523 -0.24836841\n",
      " -0.11220931 -0.23113348  0.08428943 -0.24378666  0.13307267  0.42355707\n",
      "  0.33348373 -0.34370124  0.03443677  0.18795504  0.20037192 -0.05355935\n",
      "  0.28485292  0.07176587  0.05487141 -0.08103792  0.27076888  0.11700249]\n",
      "\n",
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "Embedding: [ 0.5897932  -0.23598261 -0.25411695  0.00311613 -0.08485725 -0.2679976\n",
      " -0.07506666 -0.30021372  0.05151652  0.16585384  0.2607674   0.38256374\n",
      "  0.43732858 -0.09301949 -0.2656879  -0.09716302 -0.48096082  0.11878306\n",
      "  0.13675478  0.0471206  -0.23696548 -0.5233235  -0.01631864  0.06127283\n",
      " -0.74333    -0.11898901 -0.78865314 -0.4810885   0.10314914 -0.3237244\n",
      "  0.8144369  -0.39774528 -0.5031555  -0.79724604 -0.63248277  0.3232096\n",
      " -0.38419425 -0.11186707 -0.13243553  0.02069722 -0.14309551 -0.03701159\n",
      "  0.06116607  0.16332905 -0.11174313  0.25234276 -1.0464076  -0.37252352\n",
      "  0.15602003 -0.299916    0.19883867  0.23433447 -0.37025785  0.31733575\n",
      "  0.844286    0.06977715  0.03273654  0.09948356 -0.311413    0.50517714\n",
      "  0.00309246  0.38013673  0.0458276   0.00633391 -0.00142939 -0.13568686\n",
      " -0.07611397 -0.25844267 -0.8022129   0.55085874 -0.0912436  -0.21782015\n",
      " -0.7881091  -0.5118384   0.46672532  0.55274695 -0.37124726 -0.18645357\n",
      "  0.35856995 -0.19586325  0.18042539 -0.42548904 -0.09681419 -0.05536841\n",
      "  0.524893    0.24481152  0.01934658 -0.29637912 -0.12777822 -0.3053494\n",
      "  0.4534936   0.07469102 -0.07061692  0.26243007  0.37383923  0.14306377\n",
      "  0.00127883 -0.41776097 -0.24014115 -0.25093523  0.34843776  0.31144044\n",
      "  0.08087333 -0.5764053   0.5408527  -0.01802201 -0.12959804 -0.07399663\n",
      "  0.39369783  0.6488386  -0.0203     -0.5665558   0.29675993  0.5200022\n",
      "  0.21538723  0.10369676  0.0619923   0.01896274 -0.15269224 -1.064266\n",
      "  0.7614965   0.20734382  0.44718918  0.14494018  0.65802294 -0.09440926\n",
      " -0.23316354  0.42157093  0.11957639 -0.32571077  0.16425546 -0.49508688\n",
      " -0.19516109 -0.5618324  -0.14933239  0.610941   -0.17897928 -0.01805551\n",
      " -0.5964053   0.04918616  0.15347792 -0.4282942   0.73295265 -0.3529111\n",
      " -0.11159658  0.06127809 -0.29704425  0.43966636 -0.09660351  0.65579426\n",
      " -0.6140337   0.02576622  0.4382747   0.01733266 -0.40002277 -0.08178312\n",
      " -0.37126943  0.08230288 -0.13104396 -0.5326112  -0.2992835   0.69936585\n",
      " -0.04398743 -0.15703009  0.0979411  -0.0301746  -0.10002699  0.19996607\n",
      " -0.48188534  0.17949168  0.56566024 -0.11954793 -0.696373    0.05259662\n",
      " -0.00549602  0.16739333 -0.3169291  -0.09747505  0.33193654  0.47199655\n",
      "  0.12653966  0.19130929  0.42949095  0.5529125   0.31463325 -0.31433082\n",
      " -0.41508687  0.32897744  0.35702702 -0.19209646  0.22239406 -0.48717877\n",
      "  0.3409151  -0.2213745  -0.12667598  0.21120822 -0.313479    0.8468938\n",
      "  0.20112662 -0.42598787  0.5131571  -1.2351416   0.7697177  -0.17414242\n",
      " -0.02181134 -0.03568649 -1.1059493  -0.5720658   0.0558521   0.12461504\n",
      " -0.45065847  0.06428989 -0.1603387   0.39932907 -0.10322899 -0.02025496\n",
      " -0.18010433  0.06234777 -0.02188888 -0.15795448  0.28316954  0.02385276\n",
      "  0.03098125 -0.07853293  0.29896536 -0.06237352  0.54986775  0.17862344\n",
      "  0.21164711  0.44483352  0.04890744 -0.16238065 -0.22669889  0.18871994\n",
      "  0.07943622  0.1359757  -0.18484516  1.1135513   0.8280954  -0.31202683\n",
      "  0.09505998  0.05096114  0.38804874  0.25000468  0.5584856   0.31088772\n",
      " -0.0531859  -0.07675388  0.15282296  0.09189934 -0.01429135  0.6657542\n",
      " -0.03346026 -0.44703516  0.8006746  -0.47992834  0.17478181 -0.30563858\n",
      "  0.5536525   0.42380953  0.48674342 -0.4967799  -0.45194826 -0.9556311\n",
      " -0.2070994  -0.22605741 -0.00999152  0.98797685  0.5880777   0.08305462\n",
      " -0.5578137   0.21136868 -0.3607221   0.5266853   0.3398358  -0.15756197\n",
      "  0.00423787 -0.05354559 -0.57776755  0.5595104  -0.05747156  0.16837676\n",
      "  0.37946856 -0.25776437  0.08421452 -0.15229963 -0.03280775  0.10083845\n",
      " -0.418583   -0.44499028 -0.29309908  0.6144206   0.08548173 -0.06349564\n",
      " -0.6152554   0.7954411  -0.24058427  0.20638867 -0.51252604  0.63120145\n",
      "  0.36744308 -0.440099    0.4691398   0.23087744 -0.13737951  0.21696901\n",
      "  0.4004325  -0.0249062  -1.1396757   0.02653884 -0.32730243  0.09984156\n",
      "  0.05725678 -0.84722143  0.0645199   0.45698047  0.63563     0.45185626\n",
      " -0.2751905   0.21346192  0.17374247  0.42822054 -0.6584536   0.4000257\n",
      " -0.02035561 -0.67307854 -1.0269235   0.16877238 -0.09248724 -0.7997759\n",
      "  0.3809338   0.5171234   0.04200965 -0.04867548 -0.18772234  0.16339503\n",
      " -0.21974929  0.21939301  0.03676505 -0.29750267 -0.37409675 -0.52095056\n",
      " -0.4131463  -0.48947704 -0.81896615  0.08531483  0.3457695   0.12506028\n",
      "  0.2494524  -0.25254682 -0.0315611   0.27573124 -0.6085717   0.33569992\n",
      "  0.22913142  0.66070825 -0.3021578  -0.05315315  0.22247493  0.06138679\n",
      "  0.33555153 -0.08485162  0.08764545  0.10872062 -0.4038929  -0.14949805\n",
      "  0.19458485 -0.81060666  0.79730946 -0.41162583  0.01364123  0.23472954\n",
      " -0.09732281 -0.2904407   0.03843228 -0.07090482 -0.1740448  -0.44859397\n",
      " -0.31867254  0.41656098 -0.0543165   0.14036205  1.0559162   0.53018165]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17623387 -0.23755082 -0.25186118 ...  0.02418864 -0.05202759\n",
      "  -0.13542381]\n",
      " [-0.06796029 -0.45643526 -0.2081763  ... -0.3206765   0.10556407\n",
      "   0.02621567]\n",
      " [-0.14879279 -0.171958   -0.4128398  ... -0.02332733  0.01705487\n",
      "  -0.07746394]]\n",
      "The cat sits outside \t\t The dog plays in the garden \t\t Score: 0.2853\n",
      "A man is playing guitar \t\t A woman watches TV \t\t Score: -0.0810\n",
      "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.9298\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")\n",
    "\n",
    "embedder = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "embeddings = embedder.encode(['Hello World', 'Hallo Welt', 'Hola mundo'])\n",
    "print(embeddings)\n",
    "\n",
    "# Two lists of sentences\n",
    "sentences1 = ['The cat sits outside',\n",
    "             'A man is playing guitar',\n",
    "             'The new movie is awesome']\n",
    "\n",
    "sentences2 = ['The dog plays in the garden',\n",
    "              'A woman watches TV',\n",
    "              'The new movie is so great']\n",
    "\n",
    "#Compute embedding for both lists\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "#Compute cosine-similarits\n",
    "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "#Output the pairs with their score\n",
    "for i in range(len(sentences1)):\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d09420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/ted2020.tsv.gz does not exists. Try to download from server\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 581M/581M [00:16<00:00, 34.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/STS2017-extended.zip does not exists. Try to download from server\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 96.3k/96.3k [00:00<00:00, 1.11MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel sentences files en-sv do not exist. Create these files now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sentences: 427436it [00:26, 16377.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-08 19:05:03 - Load teacher model\n",
      "2022-02-08 19:05:03 - Load pretrained SentenceTransformer: paraphrase-distilroberta-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████| 736/736 [00:00<00:00, 178kB/s]\n",
      "Downloading: 100%|█████████████████████████| 3.74k/3.74k [00:00<00:00, 1.02MB/s]\n",
      "Downloading: 100%|██████████████████████████████| 686/686 [00:00<00:00, 173kB/s]\n",
      "Downloading: 100%|█████████████████████████████| 122/122 [00:00<00:00, 31.6kB/s]\n",
      "Downloading: 100%|████████████████████████████| 456k/456k [00:00<00:00, 938kB/s]\n",
      "Downloading: 100%|█████████████████████████████| 229/229 [00:00<00:00, 58.8kB/s]\n",
      "Downloading: 100%|███████████████████████████| 329M/329M [00:06<00:00, 51.2MB/s]\n",
      "Downloading: 100%|███████████████████████████| 53.0/53.0 [00:00<00:00, 14.0kB/s]\n",
      "Downloading: 100%|█████████████████████████████| 239/239 [00:00<00:00, 62.9kB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.36M/1.36M [00:00<00:00, 2.46MB/s]\n",
      "Downloading: 100%|██████████████████████████| 1.12k/1.12k [00:00<00:00, 285kB/s]\n",
      "Downloading: 100%|███████████████████████████| 798k/798k [00:00<00:00, 1.56MB/s]\n",
      "Downloading: 100%|█████████████████████████████| 190/190 [00:00<00:00, 47.6kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-08 19:05:24 - Use pytorch device: cpu\n",
      "2022-02-08 19:05:24 - Create student model from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████| 512/512 [00:00<00:00, 130kB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.04G/1.04G [01:10<00:00, 15.8MB/s]\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|█████████████████████████| 4.83M/4.83M [00:00<00:00, 6.03MB/s]\n",
      "Downloading: 100%|█████████████████████████| 8.68M/8.68M [00:00<00:00, 9.61MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-08 19:06:47 - Use pytorch device: cpu\n",
      "2022-02-08 19:06:47 - Load parallel-sentences/TED2020-en-sv-train.tsv.gz\n",
      "2022-02-08 19:06:48 - Create evaluator for parallel-sentences/TED2020-en-sv-dev.tsv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|                                              | 0/5 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                       | 0/3524 [00:00<?, ?it/s]\u001b[A/opt/anaconda3/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:530: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  labels = torch.tensor(labels).to(self._target_device)\n",
      "\n",
      "Iteration:   0%|                            | 1/3524 [01:05<64:06:26, 65.51s/it]\u001b[A\n",
      "Iteration:   0%|                            | 2/3524 [01:58<57:50:15, 59.12s/it]\u001b[A\n",
      "Iteration:   0%|                            | 3/3524 [02:48<54:47:38, 56.02s/it]\u001b[A\n",
      "Iteration:   0%|                            | 4/3524 [03:46<55:11:26, 56.44s/it]\u001b[A\n",
      "Iteration:   0%|                            | 5/3524 [04:31<52:41:24, 53.90s/it]\u001b[A\n",
      "Iteration:   0%|                            | 6/3524 [05:20<51:51:36, 53.07s/it]\u001b[A\n",
      "Iteration:   0%|                            | 7/3524 [06:06<50:40:50, 51.88s/it]\u001b[A\n",
      "Iteration:   0%|                            | 8/3524 [06:53<49:58:54, 51.18s/it]\u001b[A\n",
      "Iteration:   0%|                            | 9/3524 [07:33<48:29:36, 49.67s/it]\u001b[A\n",
      "Iteration:   0%|                           | 10/3524 [08:17<47:47:44, 48.97s/it]\u001b[A\n",
      "Iteration:   0%|                           | 11/3524 [09:26<49:58:20, 51.21s/it]\u001b[A\n",
      "Iteration:   0%|                           | 12/3524 [10:15<49:47:15, 51.04s/it]\u001b[A\n",
      "Iteration:   0%|                           | 13/3524 [11:07<49:51:03, 51.11s/it]\u001b[A\n",
      "Iteration:   0%|                           | 14/3524 [12:02<50:09:25, 51.44s/it]\u001b[A\n",
      "Iteration:   0%|                           | 15/3524 [12:48<49:41:50, 50.99s/it]\u001b[A\n",
      "Iteration:   0%|                           | 16/3524 [13:40<49:44:59, 51.05s/it]\u001b[A\n",
      "Iteration:   0%|▏                          | 17/3524 [14:43<50:44:03, 52.08s/it]\u001b[A\n",
      "Iteration:   1%|▏                          | 18/3524 [16:39<54:04:45, 55.53s/it]\u001b[A\n",
      "Epoch:   0%|                                              | 0/5 [16:39<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/t1/n441w3wd56d63ls3tn6j7zw00000gn/T/ipykernel_7376/1496098870.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m student_model.fit(train_objectives=[(train_dataloader, train_loss)],\n\u001b[0m\u001b[1;32m    210\u001b[0m           \u001b[0mevaluator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequentialEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_score_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    704\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                         \u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script contains an example how to extend an existent sentence embedding model to new languages.\n",
    "Given a (monolingual) teacher model you would like to extend to new languages, which is specified in the teacher_model_name\n",
    "variable. We train a multilingual student model to imitate the teacher model (variable student_model_name)\n",
    "on multiple languages.\n",
    "For training, you need parallel sentence data (machine translation training data). You need tab-seperated files (.tsv)\n",
    "with the first column a sentence in a language understood by the teacher model, e.g. English,\n",
    "and the further columns contain the according translations for languages you want to extend to.\n",
    "This scripts downloads automatically the TED2020 corpus: https://github.com/UKPLab/sentence-transformers/blob/master/docs/datasets/TED2020.md\n",
    "This corpus contains transcripts from\n",
    "TED and TEDx talks, translated to 100+ languages. For other parallel data, see get_parallel_data_[].py scripts\n",
    "Further information can be found in our paper:\n",
    "Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation\n",
    "https://arxiv.org/abs/2004.09813\n",
    "\"\"\"\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, models, evaluation, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.datasets import ParallelSentencesDataset\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sentence_transformers.util\n",
    "import csv\n",
    "import gzip\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "teacher_model_name = 'paraphrase-distilroberta-base-v2'   #Our monolingual teacher model, we want to convert to multiple languages\n",
    "student_model_name = 'xlm-roberta-base'       #Multilingual base model we use to imitate the teacher model\n",
    "\n",
    "max_seq_length = 128                #Student model max. lengths for inputs (number of word pieces)\n",
    "train_batch_size = 64               #Batch size for training\n",
    "inference_batch_size = 64           #Batch size at inference\n",
    "max_sentences_per_language = 500000 #Maximum number of  parallel sentences for training\n",
    "train_max_sentence_length = 250     #Maximum length (characters) for parallel training sentences\n",
    "\n",
    "num_epochs = 5                       #Train for x epochs\n",
    "num_warmup_steps = 10000             #Warumup steps\n",
    "\n",
    "num_evaluation_steps = 1000          #Evaluate performance after every xxxx steps\n",
    "dev_sentences = 1000                 #Number of parallel sentences to be used for development\n",
    "\n",
    "\n",
    "# Define the language codes you would like to extend the model to\n",
    "source_languages = set(['en'])                      # Our teacher model accepts English (en) sentences\n",
    "target_languages = set(['sv'])    # We want to extend the model to these new languages. For language codes, see the header of the train file\n",
    "\n",
    "\n",
    "output_path = \"output/make-multilingual-\"+\"-\".join(sorted(list(source_languages))+sorted(list(target_languages)))+\"-\"+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "# This function downloads a corpus if it does not exist\n",
    "def download_corpora(filepaths):\n",
    "    if not isinstance(filepaths, list):\n",
    "        filepaths = [filepaths]\n",
    "\n",
    "    for filepath in filepaths:\n",
    "        if not os.path.exists(filepath):\n",
    "            print(filepath, \"does not exists. Try to download from server\")\n",
    "            filename = os.path.basename(filepath)\n",
    "            url = \"https://sbert.net/datasets/\" + filename\n",
    "            sentence_transformers.util.http_get(url, filepath)\n",
    "\n",
    "\n",
    "# Here we define train train and dev corpora\n",
    "train_corpus = \"datasets/ted2020.tsv.gz\"         # Transcripts of TED talks, crawled 2020\n",
    "sts_corpus = \"datasets/STS2017-extended.zip\"     # Extended STS2017 dataset for more languages\n",
    "parallel_sentences_folder = \"parallel-sentences/\"\n",
    "\n",
    "# Check if the file exists. If not, they are downloaded\n",
    "download_corpora([train_corpus, sts_corpus])\n",
    "\n",
    "\n",
    "# Create parallel files for the selected language combinations\n",
    "os.makedirs(parallel_sentences_folder, exist_ok=True)\n",
    "train_files = []\n",
    "dev_files = []\n",
    "files_to_create = []\n",
    "for source_lang in source_languages:\n",
    "    for target_lang in target_languages:\n",
    "        output_filename_train = os.path.join(parallel_sentences_folder, \"TED2020-{}-{}-train.tsv.gz\".format(source_lang, target_lang))\n",
    "        output_filename_dev = os.path.join(parallel_sentences_folder, \"TED2020-{}-{}-dev.tsv.gz\".format(source_lang, target_lang))\n",
    "        train_files.append(output_filename_train)\n",
    "        dev_files.append(output_filename_dev)\n",
    "        if not os.path.exists(output_filename_train) or not os.path.exists(output_filename_dev):\n",
    "            files_to_create.append({'src_lang': source_lang, 'trg_lang': target_lang,\n",
    "                                    'fTrain': gzip.open(output_filename_train, 'wt', encoding='utf8'),\n",
    "                                    'fDev': gzip.open(output_filename_dev, 'wt', encoding='utf8'),\n",
    "                                    'devCount': 0\n",
    "                                    })\n",
    "\n",
    "if len(files_to_create) > 0:\n",
    "    print(\"Parallel sentences files {} do not exist. Create these files now\".format(\", \".join(map(lambda x: x['src_lang']+\"-\"+x['trg_lang'], files_to_create))))\n",
    "    with gzip.open(train_corpus, 'rt', encoding='utf8') as fIn:\n",
    "        reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "        for line in tqdm(reader, desc=\"Sentences\"):\n",
    "            for outfile in files_to_create:\n",
    "                src_text = line[outfile['src_lang']].strip()\n",
    "                trg_text = line[outfile['trg_lang']].strip()\n",
    "\n",
    "                if src_text != \"\" and trg_text != \"\":\n",
    "                    if outfile['devCount'] < dev_sentences:\n",
    "                        outfile['devCount'] += 1\n",
    "                        fOut = outfile['fDev']\n",
    "                    else:\n",
    "                        fOut = outfile['fTrain']\n",
    "\n",
    "                    fOut.write(\"{}\\t{}\\n\".format(src_text, trg_text))\n",
    "\n",
    "    for outfile in files_to_create:\n",
    "        outfile['fTrain'].close()\n",
    "        outfile['fDev'].close()\n",
    "\n",
    "\n",
    "\n",
    "######## Start the extension of the teacher model to multiple languages ########\n",
    "logger.info(\"Load teacher model\")\n",
    "teacher_model = SentenceTransformer(teacher_model_name)\n",
    "\n",
    "\n",
    "logger.info(\"Create student model from scratch\")\n",
    "word_embedding_model = models.Transformer(student_model_name, max_seq_length=max_seq_length)\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "student_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "\n",
    "###### Read Parallel Sentences Dataset ######\n",
    "train_data = ParallelSentencesDataset(student_model=student_model, teacher_model=teacher_model, batch_size=inference_batch_size, use_embedding_cache=True)\n",
    "for train_file in train_files:\n",
    "    train_data.load_data(train_file, max_sentences=max_sentences_per_language, max_sentence_length=train_max_sentence_length)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.MSELoss(model=student_model)\n",
    "\n",
    "\n",
    "\n",
    "#### Evaluate cross-lingual performance on different tasks #####\n",
    "evaluators = []         #evaluators has a list of different evaluator classes we call periodically\n",
    "\n",
    "for dev_file in dev_files:\n",
    "    logger.info(\"Create evaluator for \" + dev_file)\n",
    "    src_sentences = []\n",
    "    trg_sentences = []\n",
    "    with gzip.open(dev_file, 'rt', encoding='utf8') as fIn:\n",
    "        for line in fIn:\n",
    "            splits = line.strip().split('\\t')\n",
    "            if splits[0] != \"\" and splits[1] != \"\":\n",
    "                src_sentences.append(splits[0])\n",
    "                trg_sentences.append(splits[1])\n",
    "\n",
    "\n",
    "    #Mean Squared Error (MSE) measures the (euclidean) distance between teacher and student embeddings\n",
    "    dev_mse = evaluation.MSEEvaluator(src_sentences, trg_sentences, name=os.path.basename(dev_file), teacher_model=teacher_model, batch_size=inference_batch_size)\n",
    "    evaluators.append(dev_mse)\n",
    "\n",
    "    # TranslationEvaluator computes the embeddings for all parallel sentences. It then check if the embedding of source[i] is the closest to target[i] out of all available target sentences\n",
    "    dev_trans_acc = evaluation.TranslationEvaluator(src_sentences, trg_sentences, name=os.path.basename(dev_file),batch_size=inference_batch_size)\n",
    "    evaluators.append(dev_trans_acc)\n",
    "\n",
    "\n",
    "##### Read cross-lingual Semantic Textual Similarity (STS) data ####\n",
    "all_languages = list(set(list(source_languages)+list(target_languages)))\n",
    "sts_data = {}\n",
    "\n",
    "#Open the ZIP File of STS2017-extended.zip and check for which language combinations we have STS data\n",
    "with zipfile.ZipFile(sts_corpus) as zip:\n",
    "    filelist = zip.namelist()\n",
    "    sts_files = []\n",
    "\n",
    "    for i in range(len(all_languages)):\n",
    "        for j in range(i, len(all_languages)):\n",
    "            lang1 = all_languages[i]\n",
    "            lang2 = all_languages[j]\n",
    "            filepath = 'STS2017-extended/STS.{}-{}.txt'.format(lang1, lang2)\n",
    "            if filepath not in filelist:\n",
    "                lang1, lang2 = lang2, lang1\n",
    "                filepath = 'STS2017-extended/STS.{}-{}.txt'.format(lang1, lang2)\n",
    "\n",
    "            if filepath in filelist:\n",
    "                filename = os.path.basename(filepath)\n",
    "                sts_data[filename] = {'sentences1': [], 'sentences2': [], 'scores': []}\n",
    "\n",
    "                fIn = zip.open(filepath)\n",
    "                for line in io.TextIOWrapper(fIn, 'utf8'):\n",
    "                    sent1, sent2, score = line.strip().split(\"\\t\")\n",
    "                    score = float(score)\n",
    "                    sts_data[filename]['sentences1'].append(sent1)\n",
    "                    sts_data[filename]['sentences2'].append(sent2)\n",
    "                    sts_data[filename]['scores'].append(score)\n",
    "\n",
    "for filename, data in sts_data.items():\n",
    "    test_evaluator = evaluation.EmbeddingSimilarityEvaluator(data['sentences1'], data['sentences2'], data['scores'], batch_size=inference_batch_size, name=filename, show_progress_bar=False)\n",
    "    evaluators.append(test_evaluator)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "student_model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluation.SequentialEvaluator(evaluators, main_score_function=lambda scores: np.mean(scores)),\n",
    "          epochs=num_epochs,\n",
    "          warmup_steps=num_warmup_steps,\n",
    "          evaluation_steps=num_evaluation_steps,\n",
    "          output_path=output_path,\n",
    "          save_best_model=True,\n",
    "          optimizer_params= {'lr': 2e-5, 'eps': 1e-6, 'correct_bias': False}\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4a48e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python397jvsc74a57bd040d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
